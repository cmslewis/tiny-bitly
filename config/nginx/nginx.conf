# Production nginx configuration for web-scale (1000+ RPS, 100M DAU)
# Based on: https://www.nginx.com/blog/tuning-nginx/

# Number of worker processes (auto = number of CPU cores)
# For production: set to number of CPU cores for optimal performance
worker_processes auto;

# Error log level (warn for production, debug for troubleshooting)
error_log /var/log/nginx/error.log warn;

# PID file
pid /var/run/nginx.pid;

events {
    # Maximum connections per worker (10k is safe for most servers)
    # Formula: worker_processes * worker_connections = max connections
    # With 4 cores and 10k connections = 40k concurrent connections
    worker_connections 10000;
    
    # Use epoll on Linux for better performance (auto on other platforms)
    use epoll;
    
    # Accept multiple connections at once
    multi_accept on;
}

http {
    # MIME types
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Logging format (optimized for production)
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'rt=$request_time uct="$upstream_connect_time" '
                    'uht="$upstream_header_time" urt="$upstream_response_time"';

    # Access log (can disable for highest performance, but useful for monitoring)
    access_log /var/log/nginx/access.log main buffer=32k flush=5s;

    # Performance optimizations
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    keepalive_requests 10000;  # Reuse connections for many requests
    types_hash_max_size 2048;
    server_tokens off;  # Security: don't reveal nginx version

    # Gzip compression (reduces bandwidth, improves latency)
    gzip on;
    gzip_vary on;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;
    gzip_min_length 1000;

    # Rate limiting zones (for DDoS protection)
    # Define zones for rate limiting
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=100r/s;
    limit_req_zone $binary_remote_addr zone=read_limit:10m rate=1000r/s;
    limit_req_zone $binary_remote_addr zone=write_limit:10m rate=100r/s;  # Increased for warmup/load testing

    # Connection limiting (prevent connection exhaustion)
    # Increased for load testing - in production, you might want lower limits
    limit_conn_zone $binary_remote_addr zone=conn_limit:10m;
    limit_conn conn_limit 1000;  # Increased from 100 for high concurrency testing

    # Resolver for upstream hostname resolution (force IPv4)
    resolver 127.0.0.11 ipv4=on valid=30s;  # Docker's internal DNS, IPv4 only
    
    # Upstream backend servers
    upstream backend {
        # Use least_conn for better load distribution under high load
        least_conn;
        
        # Keep-alive connections to backends (critical for performance)
        keepalive 500;  # Increased for high throughput
        keepalive_requests 10000;
        keepalive_timeout 60s;
        
        # Server instances with health checks
        # host.docker.internal resolves to the host machine (works with IPv4 resolver)
        server host.docker.internal:8081 max_fails=3 fail_timeout=10s weight=1;
        server host.docker.internal:8082 max_fails=3 fail_timeout=10s weight=1;
        server host.docker.internal:8083 max_fails=3 fail_timeout=10s weight=1;
        server host.docker.internal:8084 max_fails=3 fail_timeout=10s weight=1;
        
        # Add more servers as needed for horizontal scaling
        # server host.docker.internal:8085 max_fails=3 fail_timeout=10s weight=1;
    }
    
    # Common proxy settings (reusable)
    map $http_upgrade $connection_upgrade {
        default upgrade;
        ''      close;
    }

    # Main server block
    server {
        listen 8080;
        server_name _;

        # Increase buffer sizes for high throughput
        client_body_buffer_size 128k;
        client_max_body_size 1m;
        client_header_buffer_size 4k;
        large_client_header_buffers 4 16k;

        # Timeouts (optimized for API)
        client_body_timeout 12;
        client_header_timeout 12;
        send_timeout 10;

        # Proxy settings
        # Increased timeouts to handle slow database queries under load
        proxy_connect_timeout 10s;
        proxy_send_timeout 60s;   # Increased from 10s for slow backends
        proxy_read_timeout 60s;   # Increased from 10s for slow database queries
        proxy_buffering on;
        proxy_buffer_size 8k;
        proxy_buffers 16 8k;
        proxy_busy_buffers_size 16k;
        proxy_temp_file_write_size 16k;

        # Health check endpoint (no rate limiting)
        location /nginx-health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }

        # Metrics endpoint (for monitoring)
        location /metrics {
            # Optional: add authentication here
            proxy_pass http://backend;
            proxy_set_header Host $host;
        }

        # Common proxy configuration (applied to all backend requests)
        proxy_http_version 1.1;
        proxy_set_header Connection "";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # Read requests (GET /{shortCode}) - higher rate limit
        # Matches 6 or more alphanumeric characters (short codes, max 30 handled by app)
        location ~ "^/[a-zA-Z0-9]{6,}$" {
            # Rate limiting (1000 req/s per IP for reads)
            limit_req zone=read_limit burst=1000 nodelay;
            limit_conn conn_limit 500;

            proxy_pass http://backend;
            
            # Cache headers (for CDN if used)
            add_header Cache-Control "public, max-age=3600" always;
        }

        # Write requests (POST /urls) - lower rate limit
        location = /urls {
            # Rate limiting (100 req/s per IP for writes, with burst for warmup)
            limit_req zone=write_limit burst=100 nodelay;
            limit_conn conn_limit 100;

            proxy_pass http://backend;
        }

        # Health/version endpoints - no rate limiting
        location ~ ^/(health|ready|version)$ {
            proxy_pass http://backend;
            # Minimal headers for health checks
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }

        # Default location (catch-all with general rate limiting)
        location / {
            # General API rate limit (100 req/s per IP)
            limit_req zone=api_limit burst=20 nodelay;
            limit_conn conn_limit 20;

            proxy_pass http://backend;
        }
    }
}
